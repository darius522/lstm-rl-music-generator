{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### __Darius Petermann__ <br>\n",
    "Email: dariusarthur.petermann01@estudiant.upf.edu <br>\n",
    "Project Git Page: https://github.com/darius522/lstm_rl_music_generator.git\n",
    "\n",
    "How this notebook works:\n",
    "\n",
    "This notebook is divided into two main sections:\n",
    "* 1. [LSTM Training Stage](#part1)\n",
    "* 2. [Deep Q-Learning Stage](#part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingus.midi import fluidsynth\n",
    "import mingus.core.notes as notes\n",
    "from mingus.containers import NoteContainer\n",
    "from mingus.containers import Note\n",
    "import time\n",
    "\n",
    "def play_sequence(seq):\n",
    "\n",
    "    SF2 = './1.sf2'\n",
    "    fluidsynth.init(SF2,'alsa')\n",
    "    time.sleep(1)\n",
    "    for pitch in seq:\n",
    "        note = Note(notes.int_to_note(pitch), 4)\n",
    "        fluidsynth.play_Note(note)\n",
    "        time.sleep(0.25)\n",
    "        fluidsynth.stop_Note(note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>LSTM Training Stage</center></h1>\n",
    "\n",
    "***\n",
    "\n",
    "### Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [01:18<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from music21 import *\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Since music21 only gives note name, we need a dict that converts name to categorical data\n",
    "note2cat = {'C':0,'C#':1,'D-':1,'D':2,'D#':3,'E-':3,'E':4,'F':5,'F#':6,\n",
    "             'G-':6,'G':7,'G#':8,'A-':8,'A':9,'A#':10,'B-':10,'B':11}\n",
    "\n",
    "all_notes = []\n",
    "for file in tqdm(glob.glob(\"./midifiles/*.mid\")):\n",
    "    midi = converter.parse(file)\n",
    "    parts = instrument.partitionByInstrument(midi)\n",
    "    for part in parts:\n",
    "        nn = part.flat.notes.stream()\n",
    "        for n in nn:\n",
    "            if not isinstance(n, chord.Chord):\n",
    "                all_notes.append(note2cat[n.name])\n",
    "        \n",
    "print(len(all_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Shape: (108655,)\n",
      "Training Data Shape: (108605, 50, 1)\n",
      "Ground Truth Data Shape: (108605, 12)\n"
     ]
    }
   ],
   "source": [
    "# Sequence length ()\n",
    "SEQ_LEN = 50\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(all_notes) - SEQ_LEN, 1):\n",
    "    seq = all_notes[i:i + SEQ_LEN] # Take the input sequence\n",
    "    out = all_notes[i + SEQ_LEN]   # Take the output note\n",
    "    X.append(seq)\n",
    "    y.append(out)\n",
    "\n",
    "X = np.asarray(X)\n",
    "X = np.reshape(X, (np.shape(X)[0],np.shape(X)[1],1))\n",
    "y_onehot = np.asarray(to_categorical(y))\n",
    "# Printing Shapes\n",
    "print('Initial Data Shape: '+str(np.shape(all_notes)))\n",
    "print('Training Data Shape: '+str(np.shape(X)))\n",
    "print('Ground Truth Data Shape: '+str(np.shape(y_onehot)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an LSTM Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(network_input):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        256,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(12))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_hdf5 = './lstm_weight-119-0.0526.hdf5'\n",
    "\n",
    "if path_to_hdf5 == '':\n",
    "    filepath = \"lstm_weight-{epoch:02d}-{loss:.4f}.hdf5\"    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss', \n",
    "        verbose=0,        \n",
    "        save_best_only=True,        \n",
    "        mode='min'\n",
    "    )    \n",
    "    callbacks_list = [checkpoint]\n",
    "    model = createModel(X)\n",
    "    model.fit(X, y_onehot, epochs=60, batch_size=512, callbacks=callbacks_list)\n",
    "else:\n",
    "    model = createModel(X)\n",
    "    model.load_weights(path_to_hdf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The very first sequence is cherry-picked from our training set\n",
    "test_sequence   = X[np.random.randint(0, np.shape(X)[0]-1)]\n",
    "pred_sequence   = []\n",
    "TEST_SEQ_LENGTH = 16\n",
    "\n",
    "for i in range(TEST_SEQ_LENGTH):\n",
    "    test_sequence = np.reshape(test_sequence, (1, len(test_sequence), 1))\n",
    "    # We predict the next note : Output will be a one-hot will prob as value so we can argmax\n",
    "    prob = model.predict(test_sequence, verbose=0)\n",
    "    pred_sequence.append(np.argmax(prob))\n",
    "    # 4. The next sequence will add this prediction to the previous one and forget its oldest value\n",
    "    test_sequence = np.append(test_sequence[:,1:SEQ_LEN,:],pred_sequence[-1])\n",
    "\n",
    "play_sequence(pred_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Deep Q-Learning Stage</center></h1>\n",
    "We feed our environement with sequences generated by our pretrained LSTM. The reward system will be based off the below music theory rules (very basic for now)\n",
    "\n",
    "***\n",
    "\n",
    "First let our total future discounted reward be:\n",
    "\n",
    "\\begin{align}\n",
    "R_t = \\sum^T_{t’=t}\\gamma^{t’-t}r_{t’}\n",
    "\\end{align}\n",
    "\n",
    "As explained in the companion slides of this notebook, this reward will be calculated off the sequences generated by our LSTM model.\n",
    "\n",
    "We will be trying to maximize this reward by learning a function \\\\(Q\\\\), which will be giving us the best action \\\\(a\\\\) for a given state \\\\(s\\\\). This mechanism can be defined as follow:\n",
    "\n",
    "\\begin{align}\n",
    "Q(s, a) = max_\\pi \\mathbb{E}[R_t|s_t = s, a_t = a, \\pi]\n",
    "\\end{align}\n",
    "\n",
    "These different well-known aspects of Q learning can be musically (for our case) interpreted as follow:\n",
    "\n",
    "* \\\\(s\\\\): current state of the composition\n",
    "* \\\\(a\\\\): latest note event generated by the LSTM\n",
    "* \\\\(r\\\\): reward given by q learning based off music theory rules\n",
    "* \\\\(a\\\\): rectified action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use gym-minigrid install from the local master code with : pip3 install -e . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from insoco.Environment import Environment\n",
    "from insoco.Plotting import plotQ\n",
    "\n",
    "from insoco.FunctionApprox import Q_function, Q_function_count\n",
    "\n",
    "# major\n",
    "KEY = [0,2,4,5,7,9,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default max_steps per episode: 100000\n",
      "Default number of agents: 1\n",
      "No environment found\n"
     ]
    }
   ],
   "source": [
    "#@Todo: Define environement configuration for our music reward policy\n",
    "conf = {\"name\":\"Composition\", \"stats\":{}}\n",
    "env = Environment(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Todo: Implement the reward policy system\n",
    "def evaluate_model(num_trials=100):\n",
    "    \n",
    "     \"\"\"Used to evaluate the rewards the model receives.\n",
    "    Generates num_trials compositions and computes the LSTM sequence and music\n",
    "    theory rewards.\n",
    "    \n",
    "    Args:\n",
    "      num_trials: The number of compositions to use for evaluation.\n",
    "    \"\"\"\n",
    "        \n",
    "    # 1. The very first sequence is cherry-picked from our training set\n",
    "    test_sequence   = X[np.random.randint(0, np.shape(X)[0]-1)]\n",
    "    pred_sequence   = []\n",
    "    TEST_SEQ_LENGTH = 16\n",
    "\n",
    "    for i in range(TEST_SEQ_LENGTH):\n",
    "        test_sequence = np.reshape(test_sequence, (1, len(test_sequence), 1))\n",
    "        # We predict the next note : Output will be a one-hot will prob as value so we can argmax\n",
    "        prob = model.predict(test_sequence, verbose=0)\n",
    "        pred_sequence.append(np.argmax(prob))\n",
    "        # 4. The next sequence will add this prediction to the previous one and forget its oldest value\n",
    "        test_sequence = np.append(test_sequence[:,1:SEQ_LEN,:],pred_sequence[-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ToDo: Update environement variables based off the reward/penalty\n",
    "def collect_reward(obs, action):\n",
    "    \"\"\" The reward policy are all called from this function. \n",
    "    Their reward/penalty amounts are gathered here based on the current state of the LSTM sequence\n",
    "    \n",
    "    Args:\n",
    "      obs: the observed note.\n",
    "      action: the chosen action.\n",
    "    Returns:\n",
    "      Float reward value.\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    \n",
    "    reward_music_theory()\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our reward policy is solely based off music theory (at least for now, as an initial step). Weĺl start simple by relying on what the pre-trained LSTM generates, and only rectify if the new action note \\\\(a\\\\) is out of a pre-defined scale. The reward can be defined as follow:\n",
    "\n",
    "\\begin{align}\n",
    "r_t(a,s) = \\frac{1}{c}r_{MT}(a,s)\n",
    "\\end{align}\n",
    "\n",
    "Where \\\\(c\\\\) is a pre-defined constant controlling how much emphasis should be put on the music theory policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ToDo: Observe composition state and reward/penalize accordingly\n",
    "def reward_music_theory(action, penalty_amount=-1.0):\n",
    "    reward = 0\n",
    "\n",
    "    action_note = self.comp[-1] # @ToDo: Define Global Var. for composition\n",
    "    \n",
    "    if action_note not in key:\n",
    "        reward = penalty_amount\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
